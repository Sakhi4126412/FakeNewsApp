# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SG_jhHVlQ-tieud5MojSs4tg2DTv95KI
"""

import streamlit as st
import pickle
import nltk
import string
import spacy
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from textblob import TextBlob

# =========================
# Load NLP Resources
# =========================
nltk.download("punkt")
nltk.download("wordnet")
nltk.download("stopwords")
nlp = spacy.load("en_core_web_sm")

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words("english"))

# =========================
# Load Saved Models
# =========================
def load_pickle(file):
    with open(file, "rb") as f:
        return pickle.load(f)

models = {
    "Lexical": load_pickle("model_lexical.pkl"),
    "Syntactic": load_pickle("model_syntax.pkl"),
    "Semantic": load_pickle("model_semantic.pkl"),
    "Discourse": load_pickle("model_discourse.pkl"),
    "Pragmatic": load_pickle("model_pragmatic.pkl"),
}

vectorizers = {
    "Lexical": load_pickle("vectorizer_lexical.pkl"),
    "Syntactic": load_pickle("vectorizer_syntax.pkl"),
    "Semantic": load_pickle("vectorizer_semantic.pkl"),
    "Discourse": load_pickle("vectorizer_discourse.pkl"),
    "Pragmatic": load_pickle("vectorizer_pragmatic.pkl"),
}

# =========================
# Feature Extractors
# =========================
def lexical_preprocess(text):
    tokens = nltk.word_tokenize(text.lower())
    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words and w not in string.punctuation]
    return " ".join(tokens)

def syntactic_features(text):
    doc = nlp(text)
    pos_tags = " ".join([token.pos_ for token in doc])
    return pos_tags

def semantic_features(text):
    blob = TextBlob(text)
    return f"{blob.sentiment.polarity} {blob.sentiment.subjectivity}"

def discourse_features(text):
    sentences = nltk.sent_tokenize(text)
    return f"{len(sentences)} {' '.join([s.split()[0] for s in sentences if len(s.split())>0])}"

pragmatic_words = ["must", "should", "might", "could", "will", "?", "!"]
def pragmatic_features(text):
    features = []
    for w in pragmatic_words:
        features.append(str(text.lower().count(w)))
    return " ".join(features)

# =========================

!pip install streamlit

!pip install nltk
import nltk

# Download the resources you need
nltk.download('punkt')
nltk.download('stopwords')